{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vs8xHHc1hsO"
      },
      "source": [
        "# Quantum Machine Learning challenge\n",
        "\n",
        "Welcome to the Quantum Machine Learning challenge of the MQST Qiskit Fall Fest!\n",
        "In this challenge, you will explore how quantum computing (QC) and machine learning (ML) can be brought together in the exciting field of Quantum Machine learning (QML). You will start using quantum computers for a simple classical classification task, and your fundamental challenge will be to design a \"Quantum Neural Network\" (QNN) that classifies input quantum data according to the phase it corresponds to.\n",
        "\n",
        "This challenge is divided in two parts:\n",
        "- Deal with a classical classification problem with a parametrized quantum computer. You will be asked to encode and process the data on a paramtrized quantum computer, and finally train it classically to attempt to solve the problem.\n",
        "- Deal with a quantum classification task, to learn to identify the phases, and reconstruct the phase diagram, of a quantum spin chain. You will reuse the process from the previous section, but instead of embeding classical data into the quantum computer, you will now input the spin states as the initial state of the quantum computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukKywLp31hsS"
      },
      "source": [
        "#### First install the necessary packages if you haven't already:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGKx3DhX1hsS"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install the necessary packages\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install numpy==2.3.4\n",
        "%pip install scipy==1.16.2\n",
        "%pip install matplotlib==3.10.7\n",
        "%pip install jupyter\n",
        "%pip install qiskit[visualization]==2.2\n",
        "# # For Local Linux or Mac, you can use the following command:\n",
        "# # %pip install 'qiskit[visualization]'\n",
        "%pip install qiskit_algorithms==0.4.0\n",
        "%pip install qiskit_aer==0.17.2\n",
        "%pip install qiskit_ibm_runtime==0.41.1\n",
        "%pip install qiskit_ibm_transpiler==0.14.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsOUWVgu1hsU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2clrSzef1hsU"
      },
      "source": [
        "## 1.Introduction\n",
        "\n",
        "This introduction and gives a brief overview of the machine learning idea that is considered:\n",
        "\n",
        "If not of machine learning, everyone has heard of artificial intelligence. From humbler origins, to the current large language models, machine learning has permeated everything and become widespread. We will focus on the supervised learning paradigm. Simplifying, we look to solve a problem where we want to assign to each input $x$ some corresponding output $y = f(x)$. In fact this is a very general way to see any problem, so we note two distinct examples:\n",
        "- An example is finding how many prime factors an integer has. This is a simple problem in the sense that we **know** how to express the $f$ that solves it, although that does not mean it is easy to do so efficiently. In this case, it has only been after Shor's quantum algorithm that this problem, previously thought of as computationally hard, has been \"broken\".\n",
        "- A much different example is the task of classifying pictures of cats and dogs. In this case we know, or rather we assume, that there is some function that solves the problem because **we** can solve it. We can manually look at pictures and tell if it contains a cat or a dog. In spite of this, no one is able to come up with a simple and direct computer algorithm to solve this task.\n",
        "\n",
        "It is for these latter kind of problems where supervised learning shines the most. The fundamental principle is that instead of trying to find this goal function $f$ directly, we take a generic parametrized guess. Now, exploiting that we can previously prepare some sample inputs/outputs (such as prelabelled cats and dogs pictures), we tweak our parameters to find the best fit. In fact, you have already done supervised learning before, as taking a linear regression is technically just that, where the guess (ansatz) is a linear function. Similarly, just as a linear regression will not directly work for a non linear problem, this choice of our ansatz is important for each problem.\n",
        "\n",
        "On the other hand, quantum computing has emerged to find the physical limit of information processing, and ideas of combining it with machine learning happened soon after. There are many ways one can quantumly enhance machine learning, such as doing some computational-heavy steps on a possibly faster quantum computer. We will instead focus on using a parametrized quantum computer as ansatz for the problem solving function $f$, a so-called (abusing language) Quantum Neural Network. Your goal during this whole challenge is to design good models for quantum machine learning, which is still an ongoing research question while quantum resources are limited to perform big scale tests.\n",
        "\n",
        "First you will apply it for a classical problem, which will probably turn out bad! Even if Quantum Computers can do some tasks better, it is important to realise this is not an actually easy goal to achieve. Quantum intuition can differ a lot from classical intuition, so it is hard to come up with good designs. Finally, you will apply it for Quantum Phase Recongition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3VqSn_-1hsU"
      },
      "source": [
        "### A simple task\n",
        "\n",
        "Consider the following classical data points, which are classified into one of two classes. Our goal is to learn to perform this classification, not by memorizing every point but rather by learning the underlying rule, so we will able to classify new points too. Below this data is generated and you can see it ploted, with two different classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttti7OO_1hsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Set random seed so results are repeatable\n",
        "np.random.seed(2025)\n",
        "\n",
        "## Underlying classification, outside of square or inside\n",
        "def underlying_function(R: np.ndarray) -> float:\n",
        "    return np.max(np.abs(R), axis=1) > 1/np.sqrt(2)\n",
        "\n",
        "\n",
        "## Prepare random dataset\n",
        "Nsamples = 500\n",
        "R = 2*np.random.random((Nsamples,2)) - np.array([1,1])  # Data points, uniform in [-1,1]x[-1,1]\n",
        "\n",
        "y = underlying_function(R)   # Classification\n",
        "\n",
        "R_class1 = R[y]                    # Class +1: Outside of square\n",
        "R_class0 = R[np.logical_not( y )]  # Class  0: Inside  of square\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.scatter(R_class1[:,0], R_class1[:,1], label=\"Class  1\")\n",
        "plt.scatter(R_class0[:,0], R_class0[:,1], label=\"Class  0\")\n",
        "plt.legend(loc = \"upper left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbiZy6Ht1hsV"
      },
      "source": [
        "The underlying function is simple: points inside the $\\sqrt{2}$ square are in class 0 and points outside in class 1, but we must now try to learn this in a generic way, even if this is a simple case.\n",
        "We will learn to classify points, by learning the underlying distribution, which you can see is $f(\\vec{x}) = 0$, if $\\vec{x} \\in [-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}] \\times [-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}] $; $\\quad 1$ otherwise. To do this, you will create a Quantum Neural Network (QNN) and later you will train it on the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vQjfj9r1hsV"
      },
      "source": [
        "### 1.1 Classical to Quantum\n",
        "\n",
        "Let's create the Quantum Neural Network. First, encode your problem, your data, into the quantum computer. Do it for a very simple 2 qubit QC.\n",
        "> Hint: As an example you may use Qiskit's `qiskit.circuit.library.ZZFeatureMap` or `ZFeatureMap` to encode real numbers as angles of rotation on each qubit\n",
        "\n",
        "You will also want to define some processing layers that use trainable parameters.\n",
        "> Hint: If you do not want to manually construct them, with gates, you may use provided examples like `qiskit.circuit.library.EfficientSU2` or `qiskit.circuit.library.TwoLocal`\n",
        "\n",
        "The principle is to use gates, such as $R_Y$ rotations, where the angles will be given according to some input/tunable parameters\n",
        "\n",
        "\n",
        "**The code snippets are orientative to guide you through the problems, but feel free to come up with your own solutions**. For instance, you may mix input parameters and trainable parameters on the same gate, or intertwine gates (also called data re-reuploading) instead of applying encoding first and processing later. Some of these ideas will also be proposed later\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf49BU-w1hsV"
      },
      "outputs": [],
      "source": [
        "## Your code here\n",
        "import qiskit\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit import Parameter, ParameterVector\n",
        "from qiskit.circuit.library import ZZFeatureMap, ZFeatureMap     # Example featuremap/encoding\n",
        "from qiskit.circuit.library import EfficientSU2, RealAmplitudes  # Example processing layer\n",
        "\n",
        "# Example of parameters you may use. Default input. If using default, inputs are called x and trainable_parameters θ\n",
        "inputs = ParameterVector('x', 2)\n",
        "trainable_parameters = ParameterVector('θ', 16)\n",
        "\n",
        "# By default, you may use only 2 qubits, one for each coordinate\n",
        "\n",
        "def encoding(inputs: ParameterVector) -> QuantumCircuit:\n",
        "    \"\"\"Returns an encoder that will encode a point in space (x,y) into a quantum circuit.\n",
        "\n",
        "    Args:\n",
        "        inputs (ParameterVector): parameters corresponding to input (x, y)\n",
        "\n",
        "    Returns:\n",
        "        encoding_circuit (QuantumCircuit): Returns a QuantumCircuit, which encodes the classical data into the circuit\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return encoding_circuit\n",
        "\n",
        "\n",
        "def processing(trainable_parameters: ParameterVector) -> QuantumCircuit:\n",
        "    \"\"\"Returns a processing layer that will be trained to try to solve our problem\n",
        "\n",
        "    Args:\n",
        "        trainable_parameters (ParameterVector): parameters that will be trained to attempt to solve the problem\n",
        "\n",
        "    Returns:\n",
        "        processing_circuit (QuantumCircuit): Returns a QuantumCircuit that encodes the data into the circuit\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return processing_circuit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdKLD7tg1hsW"
      },
      "source": [
        "Here you may draw your encoding,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4x-WbNY1hsW"
      },
      "outputs": [],
      "source": [
        "encoding_circuit = encoding(inputs)\n",
        "encoding_circuit.decompose().draw(output='mpl', style='textbook')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5S0uZ8D1hsW"
      },
      "source": [
        "processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKuCTPUl1hsW"
      },
      "outputs": [],
      "source": [
        "processing_circuit = processing(trainable_parameters)\n",
        "processing_circuit.decompose().draw(output='mpl', style='textbook')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8iIXR_i1hsW"
      },
      "source": [
        "and their combination:\n",
        "> Again, remember you can join encoding and processing layers differently, or mix them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJsTNnDm1hsW"
      },
      "outputs": [],
      "source": [
        "# Assign our parameters, may need to adapt\n",
        "encoding_circuit.assign_parameters(inputs, inplace=True)\n",
        "processing_circuit.assign_parameters(trainable_parameters, inplace=True)\n",
        "\n",
        "# Define our QNN Circuit\n",
        "qnn_circuit = encoding_circuit.compose(processing_circuit)\n",
        "qnn_circuit.draw(output='mpl', style='textbook')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLHHzpQp1hsX"
      },
      "source": [
        "### 1.2. Finish your QNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNT-nm6-1hsX"
      },
      "source": [
        "You have prepared a quantum circuit, parametrized, that will attempt to classify our data, you now need to define how to interpret the circuit outputs to do the classification. We can do so by reading the output, for instance the first qubit, and using and assigning the class according to the result (measuring 0 or measuring 1). Moreover, you can treat the output as the probability vector of the different outcomes, and train it accordingly.\n",
        "\n",
        "\n",
        "Keep in mind that Qiskit uses Little Endian notation (top qubit q[0] is last qubit and so on $|abc\\rangle \\to$ q[0]=c, q[1]=b, q[2]=a), and that the default sampler output is the integer representation of the bitstring you need to interpret.\n",
        "\n",
        "To do this, define a forward function, to evaluate running our QNN.\n",
        "\n",
        "> Again, all function suggestions are orientative. If you prefer you may also work with Estimators (expectation values) instead of Samplers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MWOkrqb1hsX"
      },
      "outputs": [],
      "source": [
        "#from qiskit_machine_learning.neural_networks import EstimatorQNN, SamplerQNN    # Libraries which could simplify process, but icompatible with qiskit 2>\n",
        "#from qiskit_machine_learning.algorithms import NeuralNetworkClassifier\n",
        "from qiskit_ibm_runtime import Sampler\n",
        "from qiskit_aer import AerSimulator\n",
        "from qiskit_algorithms.gradients import ParamShiftSamplerGradient\n",
        "\n",
        "\n",
        "# Define a forward pass of your QNN\n",
        "def forward(qnn_circuit: QuantumCircuit, inputs: np.ndarray, trainable_parameters: np.ndarray, sampler: Sampler) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    \"Forward pass of the Q neural network\", returns output probability vector of circuit (or other way that occurs to you, for insance expected value of observable)\n",
        "\n",
        "    Args:\n",
        "        qnn_circuit (QuantumCircuit): circuit to run, including some input encoding and trainable processing gates.\n",
        "        inputs (np.ndarray): data inputs encoded.\n",
        "        trainable_parameters (np.ndarray): neural network ansatz parameters.\n",
        "        sampler (Sampler): Sampler to use.\n",
        "\n",
        "    Returns:\n",
        "        qnn_output (np.ndarray): Array for the probability distribution or samples of measurement outcomes of circuit\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return qnn_output\n",
        "\n",
        "# Define a backward pass of your QNN. Optional, but useful and necessary to use later for minimization procedures that require computing derivatives\n",
        "def backwards(qnn_circuit: QuantumCircuit, inputs: np.ndarray, trainable_parameters: np.ndarray, sampler: Sampler) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    \"Backwards pass of the Q neural network\", compute gradients of parameters using ParamShiftSamplerGradient.\n",
        "\n",
        "    Args:\n",
        "        qnn_circuit (QuantumCircuit): circuit to run, including some input encoding and trainable processing gates.\n",
        "        inputs (np.ndarray): data inputs encoded.\n",
        "        trainable_parameters (np.ndarray): neural network ansatz parameters.\n",
        "        sampler (Sampler): Sampler to use.\n",
        "\n",
        "    Returns:\n",
        "        qnn_output (np.ndarray): Array for the probability distribution or samples of measurement outcomes of circuit\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return qnn_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxJsecvY1hsX"
      },
      "source": [
        "### 1.3 Train your QNN\n",
        "\n",
        "Now you must train your QNN, to do this define a loss funciton and an optimizer.\n",
        "\n",
        "\n",
        "For instance, use a mean squared error loss function of the form\n",
        "$$\\sum (QNN_{\\vec{\\theta}}(\\vec{x_i})-y_i)^2,$$\n",
        "where $QNN()$ represents the output of your quantum circuit (forward pass). If you are following the example, then the output of the QNN is the array of probabilities for each outcome (classification), so you should treat $y_i$ as a vector too (either [1,0] or [0,1])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeE3mx7_1hsX"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUUua8Tl1hsX"
      },
      "source": [
        "\n",
        "Minimize the loss, computed for your data points, to train your model. You can use scipy's minimize, with built in optimizers such as COBYLA.\n",
        "\n",
        "\n",
        "Store also the loss evolution (and other parameters you consider interesting) to plot them later.\n",
        "\n",
        "> Hint: Remember to look for Qiskit resources, for instance you may find [this](https://quantum.cloud.ibm.com/learning/en/courses/quantum-machine-learning/qvc-qnn) tutorial useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpHhojx51hsX"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define loss value_list that will be updated\n",
        "loss_value_list = []\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OULjZMz31hsY"
      },
      "source": [
        "#### 1.4 Plot your reconstructed classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRdoelXc1hsY"
      },
      "outputs": [],
      "source": [
        "def plot_prediction(QNN_classifier):\n",
        "    \"\"\"Auxiliar function to plot classification. Feel free to modify it\"\"\"\n",
        "    # Making prediction\n",
        "    prediction = QNN_classifier.predict(R)\n",
        "\n",
        "    is_one = prediction == 1\n",
        "\n",
        "    predicted_R_class1 = R[is_one]                    # Class +1: Outside of square\n",
        "    predicted_R_class0 = R[np.logical_not( is_one )]  # Class  0: Inside  of square\n",
        "\n",
        "    ## PLOT Prediction\n",
        "    plt.axis('equal')\n",
        "\n",
        "    # Draw square\n",
        "    plt.plot([-np.sqrt(2)/2, -np.sqrt(2)/2, np.sqrt(2)/2, np.sqrt(2)/2, -np.sqrt(2)/2], [-np.sqrt(2)/2, np.sqrt(2)/2, np.sqrt(2)/2, -np.sqrt(2)/2, -np.sqrt(2)/2], 'k--')\n",
        "    plt.scatter(predicted_R_class1[:,0], predicted_R_class1[:,1], label=\"Class  1\")\n",
        "    plt.scatter(predicted_R_class0[:,0], predicted_R_class0[:,1], label=\"Class  0\")\n",
        "    plt.legend(loc = \"upper left\")\n",
        "\n",
        "plot_prediction(QNN_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTm1zu-o1hsY"
      },
      "source": [
        "### 1.5 Explore\n",
        "These sections is more optional, if you are doing well you may skip to section 2 already. If you get stuck at section 2, you can get back to work on it, or work on it later, or split it with other members of the group.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxdiJ5jz1hsY"
      },
      "source": [
        "#### 1.5.1 Data re-uploading\n",
        "\n",
        "Instead of just changing the encoding layer and the processing layer, explore here if you can obtain better results by intertwining them:\n",
        "\n",
        "Build your circuit with an encoding layer, a processing layer, another encoding layer and another processing layer. As this will increase the complexity, you may also want to simplify the layers themselves. Try with different ammounts of intertwining, do you get better results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of5mzOMD1hsY"
      },
      "outputs": [],
      "source": [
        "# Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYCKsSAU1hsY"
      },
      "source": [
        "## 2. Putting more quantum in Quantum Machine Learning\n",
        "\n",
        "Right now you have worked on a classical classification problem using quantum computers. Now we switch to the main challenge.\n",
        "\n",
        "Your task will be to train a quantum machine learning model to classify which phase of matter corresponds to each input quantum state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YixJYXDT1hsY"
      },
      "source": [
        "We are going to study now the XY model on a periodic chain, with a hamiltonian representing its energy and properties given by\n",
        "\n",
        "$$H = -\\sum^N_{j=1} \\left [ \\left(\\frac{1+\\gamma}{2}\\right) X_j X_{j+1} + \\left(\\frac{1-\\gamma}{2}\\right) Y_j Y_{j+1}  + h Z_j \\right],$$\n",
        "\n",
        "with $\\sigma_{N+1}=\\sigma_1$.\n",
        "It has the following simple phase diagram, where we will focus on the $h,\\gamma>0$ region, with phase transitions at $h = 1$ and at the circular boundary $\\gamma^2 + h^2 = 1$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeD2Ort01hsY"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCcosA9W1hsY"
      },
      "source": [
        "### 2.1 Implement the hamiltonian\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DT4ESlT1hsZ"
      },
      "source": [
        "First you must implement the hamiltonian in Qiskit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFoKThI71hsZ"
      },
      "outputs": [],
      "source": [
        "from qiskit.quantum_info import SparsePauliOp\n",
        "# Code here\n",
        "\n",
        "def XY_hamiltonian(gamma: float, h: float, Nsites: int) -> SparsePauliOp:\n",
        "    \"\"\"Returns the XY hamiltonian\n",
        "\n",
        "    Args:\n",
        "        gamma (double): gamma parameter of hamiltonian\n",
        "        h (double): h transverse field parameter\n",
        "        Nsites (int): Number of spin sites, equivalent to number of qubits for a one to one mapping\n",
        "\n",
        "    Returns:\n",
        "        hamiltonian (SparsePauliOp): XY_hamiltonian with the given parameters\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJBDraE81hsZ"
      },
      "source": [
        "### 2.2 Solve it classically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O23jsAHG1hsZ"
      },
      "source": [
        "Create a classical method to find the ground states of the hamiltonian\n",
        "> hint: use libraries such as eigsh to diagonalise the hamiltonian with `.to_matrix()`\n",
        "\n",
        "The steps are similar to the case before, but now the inputs do not need to be directly embeded into the quantum computer as they are already quantum states. You can use `.prepare_state()`. You can start simple, with N=6 qubits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBwFeT9n1hsZ"
      },
      "outputs": [],
      "source": [
        "def XY_ground_state(gamma: float, h: float, Nsites: int) -> np.ndarray:\n",
        "    \"\"\"Returns the ground state for the corresponding XY hamiltonian\n",
        "\n",
        "    Args:\n",
        "        gamma (double): gamma parameter of hamiltonian\n",
        "        h (double): h transverse field parameter\n",
        "        Nsites (int): Number of spin sites, equivalent to number of qubits for a one to one mapping\n",
        "\n",
        "    Returns:\n",
        "        ground_state (np.ndarray): ground state of the corresponding hamiltonain\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2wu7QJ_1hsZ"
      },
      "source": [
        "### 2.3 Construct the QNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244eYyPA1hsZ"
      },
      "source": [
        "Construct your QNN model that will, after training, attempt to classify each input quantum state to the correct phase. You may start with a small number of qubits, 6, to check things work, and scale up later.\n",
        "\n",
        "In this case, you won't need to encode your data, as you can map each spin to a qubit directly. You can input states into qiskit circuits directly, and this is how you can deal with this challenge, with tools like `StatePreparation`. Construct your QNN here, you can adapt your processing layers you designed in section 1.2. Notice that now you have more classes, so you may want to use 2 output qubits instead of just 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtiFETj_1hsd"
      },
      "outputs": [],
      "source": [
        "from qiskit.circuit import Parameter\n",
        "from qiskit.primitives import BaseSamplerV2\n",
        "from qiskit_ibm_runtime import Sampler  # actual to use\n",
        "\n",
        "def QNN_XY_circuit(parameters: np.ndarray) -> QuantumCircuit:\n",
        "    \"\"\"Returns a Quantum neural Network model that will attempt to solve the phase classification task after trayining.\n",
        "    Args:\n",
        "        gamma (double): gamma parameter of hamiltonian\n",
        "\n",
        "    Returns:\n",
        "        QNN (np.ndarray): Quantum Neural Network that will proces your states\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw89qGJ71hse"
      },
      "source": [
        "Define as in the previous sections some forward passes and reuse or adapt the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbTJr-Li1hse"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouTkcLvX1hse"
      },
      "source": [
        "### 2.4 Train it for your quantum data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6CPJmHL1hse"
      },
      "source": [
        "Use your classical solver to find the ground states for your system at different parameters, and use those states, with their known phase, to define the loss and train your model.\n",
        "\n",
        "Finally, draw the phase diagram that your system detects, where you may find it useful to define a prediciton function for a particular input state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPLcwfzP1hsf"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qwcV8ZX1hsf"
      },
      "outputs": [],
      "source": [
        "def draw_predicted_phasediagram(predict_XY) -> None:\n",
        "    \"\"\"Plots the predicted phase diagram.\n",
        "    \"\"\"\n",
        "    prediction = []\n",
        "\n",
        "    gamma_range = np.linspace(0,2)\n",
        "    h_range = np.linspace(0,2)\n",
        "    for gamma in gamma_range:\n",
        "        prediction.append([])\n",
        "        for h in h_range:\n",
        "            prediction[-1].append(predict_XY(gamma, h))\n",
        "\n",
        "    gamma_mesh, h_mesh = np.meshgrid(gamma_range, h_range)\n",
        "    plt.contourf(gamma_mesh, h_mesh, prediction)\n",
        "    plt.show()\n",
        "\n",
        "draw_predicted_phasediagram(predict_XY)\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--B2Zvup1hsf"
      },
      "source": [
        "### 2.5 Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVrPo-Vz1hsf"
      },
      "source": [
        "Repeat the previous steps using different optimizers: SPSA/COBYLA/BFGS/ADAM and gate ansatz.\n",
        "\n",
        "> Hint: As some criterion, you can use gates related to the elements in the hamiltonian. For instance $R_X$, $R_{XX}$ gates etc. You may also consider some other properties of your system, such as symmetries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sBxiaMl1hsf"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3XapXvc1hsg"
      },
      "source": [
        "What kind of model works best, and optimizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uot8r-IV1hsg"
      },
      "source": [
        "### 2.6 Constrained training\n",
        "\n",
        "Consider that using as training data many points across the whole phase diagram is very ideal, in general you may be constrained to some easier subspaces to reproduce/generate.\n",
        "\n",
        "Perform the previous steps, training and predicting the phase diagram, but with constrained initial data points. Compare different cases:\n",
        "\n",
        "Take points across the line $gamma=1$, across the line $h = 0.5$, and across the horizontal and vertical lines crossing at $\\gamma, h = 0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8FeVz091hsg"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytO589GA1hsg"
      },
      "source": [
        "How do the results change with the training region?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN904eYL1hsg"
      },
      "source": [
        "### 3. More questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL-yxpKf1hsg"
      },
      "source": [
        "### Scalability\n",
        "The fundamental principle that motivates quantum machine learning is the search for some quantum advantage with respect to classical machine learning. To simulate a generic quantum computer requires exponential time, although smart techniques such as tensor networks can dequantize simple or very structured circuits, so the ideal criterion is to have our models require at most polynomial resources with the number of qubits. You are invited to tackle this in any way you want, where the general idea is to scale the system for different sizes and see how it fares. Next we provide an example criterion that has become standard, the barren plateau phenomena.\n",
        "\n",
        "It turns out that the expressability of quantum circuits, due to the exponential scaling of the Hilbert space, comes at a cost, many problems have gradients vanishing exponentially with number of qubits:\n",
        "\n",
        "**Compute the variance of gradients of your loss functions for random initial parameters of your circuits, and plot how these variances scale with increasing number of sites/qubits (Ex. N=6,7,8,9,10,11,12)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwuL7t351hsh"
      },
      "source": [
        ">  Hint: You can start by defining a way to compute gradients. For instance you can reuse the backwards methods, that were proposed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMF9heme1hsh"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wh8NU9k1hsh"
      },
      "source": [
        "### Unsupervised learning\n",
        "Instead of learning to classify states, having been provided with the knowledge of the existing phases, try now to discover which phases there are. That is, make an unsupervised quantum machine learning paradigm to identify the existence of the different phases the model has.\n",
        "[This paper](https://arxiv.org/abs/2402.11022) presents an approach to do this. An idea is that instead of performing a classifcation itself, and defining the loss by penalising cases where it differs from the known data, you should compress the quantum states into a smaller subsystem, training it at some point. The success of this compression, after training, which will be the output of your QNN, will be better for states similar to the initial state, and through this you may find"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9u68Pit1hsh"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9GIQNvv1hsh"
      },
      "source": [
        "### Noise and sampling\n",
        "\n",
        "Introduce sampling and noise into the mix! Do the previous results hold? Do some optimizers or architechtures become better now?\n",
        " You could try to also use IBM hardware, but it is not recommended because the state preparation, from arbitary statevectors, is not very hardware friendly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtNNBHKs1hsh"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17VMEPqd1hsh"
      },
      "source": [
        "## References\n",
        "[1]. Cerezo, M., Verdon, G., Huang, H., Cincio, L., & Coles, P. J. (2022). Challenges and opportunities in quantum machine learning. Nature Computational Science, 2(9), 567–576\n",
        "\n",
        "[2]. Bharti, K., Cervera-Lierta, A., Kyaw, T. H., Haug, T., Alperin-Lea, S., Anand, A., Degroote, M., Heimonen, H., Kottmann, J. S., Menke, T., Mok, W., Sim, S., Kwek, L., & Aspuru-Guzik, A. (2022). Noisy intermediate-scale quantum algorithms. Reviews of Modern Physics, 94(1)\n",
        "\n",
        "[3] Franchini, Fabio. An introduction to integrable techniques for one-dimensional quantum systems. Vol. 940. Cham: Springer International Publishing, 2017. ([url](https://arxiv.org/abs/1609.02100))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "qiskitfallfest2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}